# -*- coding: utf-8 -*-
"""Task2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jqf_rsY1pR6eExlZvjxjfvRXbhOubvyh

Machine Learning
Task 2:
Download the dataset from https://drive.google.com/file/d/18T3IZpr4GeoCho0MbF_bqjp1tGGHLk2A/view?usp=sharing and perform the following task :

1.Preprocess the data
2.Analyse the data and train a model that divides this data into multiple groups.
3.Determine what would be the best number of groups we should divide the data.
4.Take two columns with the highest correlation and plot a graph between those columns to display the group formation.

**1.Preprocess the data and load dataset**
"""

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
url = "https://drive.google.com/uc?export=download&id=18T3IZpr4GeoCho0MbF_bqjp1tGGHLk2A"
data = pd.read_csv(url)
print(data.head())

# Check for missing values
print(data.isnull().sum())

# Fill missing values or drop rows/columns with missing values
data = data.dropna()

# Encode categorical variables if necessary
from sklearn.preprocessing import LabelEncoder

label_encoders = {}
for column in data.select_dtypes(include=['object']).columns:
    label_encoders[column] = LabelEncoder()
    data[column] = label_encoders[column].fit_transform(data[column])

print(data.head())

"""**2.Analyse the data and train a model that divides this data into multiple groups.**"""

# Standardize the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(data)

# Determine the optimal number of clusters using the Elbow Method
inertia = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(scaled_data)
    inertia.append(kmeans.inertia_)

# Plot the Elbow Method graph
plt.figure(figsize=(8, 4))
plt.plot(range(1, 11), inertia, marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')
plt.title('Elbow Method For Optimal Number of Clusters')
plt.show()

"""**3.Determine what would be the best number of groups we should divide the data. **"""

# Based on the elbow method, we choose the number of clusters (e.g., 3)
optimal_clusters = 3
kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)
data['Cluster'] = kmeans.fit_predict(scaled_data)
print(data.head())

"""** 4.Take two columns with the highest correlation and plot a graph between those columns to display the group formation.**"""

# Calculate the correlation matrix
corr_matrix = data.corr()

# Select the top 2 columns with the highest correlation (excluding the 'Cluster' column)
corr_matrix = corr_matrix.abs()
corr_matrix = corr_matrix.unstack().sort_values(ascending=False)
corr_matrix = corr_matrix[corr_matrix.index.get_level_values(0) != corr_matrix.index.get_level_values(1)]
highest_corr = corr_matrix.index[0]

# Plot the two columns with the highest correlation
col1, col2 = highest_corr
plt.figure(figsize=(10, 6))
sns.scatterplot(x=data[col1], y=data[col2], hue=data['Cluster'], palette='viridis')
plt.title(f'Clusters based on {col1} and {col2}')
plt.xlabel(col1)
plt.ylabel(col2)
plt.show()