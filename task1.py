# -*- coding: utf-8 -*-
"""Task1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Em-W2SNR4eWJhgxcAyLMcbf33mNx58fg

Machine Learning
Task 1 :
Download the dataset from https://drive.google.com/file/d/1SMxwWg5DjKEPwdhPM_590he60S1GrfhU/view?usp=sharing and perform the following actions:

1.Preprocess the data.
2.Plot a detailed respective graph for each column.
3.Analyse the data properly and train the most suitable machine learning to analyse if the person is diabetic.
5.Test the trained model
6.Evaluate the model and tune the model to maximize the accuracy
"""

# Step 1: Preprocess the data
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.model_selection import GridSearchCV

# Load dataset
data = pd.read_csv('https://drive.google.com/uc?id=1SMxwWg5DjKEPwdhPM_590he60S1GrfhU')  # replace with your dataset path

# Show basic info
print(data.info())
print(data.describe())

# Step 2: Plot a detailed respective graph for each column
for column in data.columns:
    plt.figure(figsize=(10, 6))
    if data[column].dtype == 'object':
        sns.countplot(x=column, data=data)
    else:
        sns.histplot(data[column], kde=True)
    plt.title(f'Distribution of {column}')
    plt.show()

# Step 3: Analyse the data properly and train the most suitable machine learning model
# Identify features and target
X = data.drop('Fruits', axis=1)  # replace 'Outcome' with your target column
y = data['Fruits']  # replace 'Outcome' with your target column

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Identify numerical and categorical columns
num_features = X.select_dtypes(include=[np.number]).columns.tolist()
cat_features = X.select_dtypes(exclude=[np.number]).columns.tolist()

# Preprocessing for numerical data
num_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# Preprocessing for categorical data
cat_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combine preprocessing steps
preprocessor = ColumnTransformer([
    ('num', num_pipeline, num_features),
    ('cat', cat_pipeline, cat_features)
])

# Create a complete pipeline with the model
pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(random_state=42))
])

# Train the model
pipeline.fit(X_train, y_train)

# Step 4: Test the trained model
y_pred = pipeline.predict(X_test)
y_pred
#X_pred = pipeline.predict(y_test)
#X_pred

# Step 5: Evaluate the model and tune the model to maximize the accuracy
print("Classification Report:")
print(classification_report(y_test, y_pred))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("Accuracy Score:", accuracy_score(y_test, y_pred))

# Hyperparameter tuning using GridSearchCV
param_grid = {
    'classifier__n_estimators': [100, 200, 300],
    'classifier__max_depth': [None, 10, 20, 30],
    'classifier__min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid_search.fit(X_train, y_train)

# Best parameters and estimator
print("Best Parameters:", grid_search.best_params_)
best_model = grid_search.best_estimator_

# Evaluate the tuned model
y_pred_tuned = best_model.predict(X_test)
print("Tuned Model Classification Report:")
print(classification_report(y_test, y_pred_tuned))
print("Tuned Model Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_tuned))
print("Tuned Model Accuracy Score:", accuracy_score(y_test, y_pred_tuned))